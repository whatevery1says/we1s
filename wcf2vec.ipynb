{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1301.3781v3.pdf\n",
    "\n",
    "https://github.com/chrisjmccormick/word2vec_commented\n",
    "\n",
    "http://www.claudiobellei.com/2018/01/07/backprop-word2vec-python/\n",
    "\n",
    "https://github.com/cbellei/word2veclite\n",
    "\n",
    "https://keras.io/preprocessing/sequence/#skipgrams\n",
    "\n",
    "https://github.com/Hironsan/awesome-embedding-models/blob/master/examples/skip-gram_with_ns.py\n",
    "\n",
    "https://github.com/nzw0301/keras-examples\n",
    "\n",
    "https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/sequence.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import pickle\n",
    "import gensim\n",
    "from collections import Counter, OrderedDict\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input\n",
    "from keras.layers.merge import Dot\n",
    "from keras.utils import Sequence\n",
    "from keras.preprocessing.sequence import skipgrams, make_sampling_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73969958 0.83902403 0.44205325 0.78135131 0.93242846 0.36705956\n",
      " 0.13969671 0.94913261 0.29242238 0.85364323] (10,)\n",
      "[False False False False  True  True  True  True  True  True] (10,) 6\n",
      "[4 5 6 7 8 9] (6,)\n"
     ]
    }
   ],
   "source": [
    "sampling_table=np.array([0,0,0,0,0,1,1,1,1,1,1])\n",
    "\n",
    "ii = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "rr = np.random.random(size=ii.shape[0])\n",
    "print(rr, rr.shape)\n",
    "mm = np.array(sampling_table[ii+1] > rr, dtype=np.bool)\n",
    "print(mm, mm.shape, mm.sum())\n",
    "ii = ii[mm]\n",
    "print(ii, ii.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: implement loading of pickled wcf matrix\n",
    "# Load from pickle file\n",
    "# with open('data.pic', 'rb') as f:\n",
    "#     source = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "corpus = 'c/proust_ascii.txt'\n",
    "dim = 300 # word2vec default: 300\n",
    "window_size = 5 # word2vec default: 5\n",
    "min_count = 5 # word2vec default: 5 - set to 0 to disable\n",
    "sampling_factor = 1e-03 # word2vec default: 1e-03 - set to 0 to disable\n",
    "positive_samples = 10000 # gensim.word2vec.Text8Corpus default: 10000 words/sequence -> Keras skipgram function\n",
    "negative_samples = 5 # word2vec default: 5\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text size: 7049641 characters\n",
      "Converting text to word list...\n"
     ]
    }
   ],
   "source": [
    "# Read corpus\n",
    "with open(corpus, 'r') as f: text = f.read()\n",
    "\n",
    "# Make it all lower case    \n",
    "text = text.lower()\n",
    "\n",
    "# Remove all punctuation\n",
    "exclude = set(string.punctuation)\n",
    "text = ''.join(char for char in text if char not in exclude)\n",
    "\n",
    "print('Text size: '+ str(len(text)) + ' characters')\n",
    "\n",
    "# Truncate to a million characters for quick testing\n",
    "text = text\n",
    "\n",
    "# Make list\n",
    "print('Converting text to word list...')\n",
    "text = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 11006 words\n"
     ]
    }
   ],
   "source": [
    "# Get frequencies\n",
    "frequencies = Counter(text).most_common()\n",
    "\n",
    "# Eliminate words that appear less than min_count times\n",
    "frequencies = OrderedDict({word: i for word, i in frequencies if i >= min_count})\n",
    "\n",
    "# Map words to integers\n",
    "token2id = {word: i for i, word in enumerate(frequencies.keys())}\n",
    "id2token = {i: word for i, word in enumerate(frequencies.keys())}\n",
    "\n",
    "# Length of dictionary is size of vocabulary\n",
    "vocabulary_size = len(token2id)\n",
    "\n",
    "# Encode\n",
    "text = [token2id[token] for token in text if token in token2id]\n",
    "\n",
    "# Make sampling table for subsampling (starts at 1!)\n",
    "sampling_table = make_sampling_table(size=vocabulary_size+1, sampling_factor=sampling_factor)\n",
    "\n",
    "print('Vocabulary size: ' + str(vocabulary_size) + ' words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling WCF matrix...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5be60cfbf47468aa736d0ac7c7037d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1278853), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples in matrix: 12788500 samples\n",
      "Creating sparse matrix...\n"
     ]
    }
   ],
   "source": [
    "# Create square word-context-frequency matrix\n",
    "print('Filling WCF matrix...')\n",
    "mat = np.zeros((vocabulary_size, vocabulary_size), dtype=np.int32)\n",
    "for i, wi in (enumerate(tqdm_notebook(text))):\n",
    "    window_start = max(0, i - window_size)\n",
    "    window_end = min(len(text), i + window_size + 1)\n",
    "    for j in range(window_start, window_end):\n",
    "        if j != i:\n",
    "            wj = text[j]\n",
    "            mat[wi][wj]+=1            \n",
    "print('Total samples in matrix: ' + str(mat.sum()) + ' samples')\n",
    "        \n",
    "# Create sparse LIL representation of square word-context matrix\n",
    "print('Creating sparse matrix...')\n",
    "coo_mat = coo_matrix(mat)\n",
    "lil_mat = coo_mat.tolil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 300)       3301800     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 300)       3301800     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 1)         0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1)            0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           reshape_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,603,600\n",
      "Trainable params: 6,603,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "\n",
    "t_inputs = Input(shape=(1, ), dtype=np.int32)\n",
    "t = Embedding(vocabulary_size, dim)(t_inputs)\n",
    "c_inputs = Input(shape=(1, ), dtype=np.int32)\n",
    "c  = Embedding(vocabulary_size, dim)(c_inputs)\n",
    "o = Dot(axes=2)([t, c])\n",
    "o = Reshape((1,), input_shape=(1, 1))(o)\n",
    "o = Activation('sigmoid')(o)\n",
    "sgns = Model(inputs=[t_inputs, c_inputs], outputs=o)\n",
    "sgns.summary()\n",
    "sgns.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, testword):\n",
    "        self.testword = testword\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.most_similar(positive=[self.testword], negative=[], topn=3)\n",
    "        \n",
    "    def most_similar(self, positive, negative=[], topn=10):\n",
    "        \n",
    "        # Gensim word vector file format\n",
    "        with open('vectors.txt' ,'w') as f:\n",
    "            f.write('{} {}\\n'.format(vocabulary_size, dim))\n",
    "            vectors = sgns.get_weights()[0]\n",
    "            for token, i in token2id.items():\n",
    "                f.write('{} {}\\n'.format(token, ' '.join(map(str, list(vectors[i,:])))))\n",
    "                \n",
    "        # Load and get most similar vectors\n",
    "        w2v = gensim.models.KeyedVectors.load_word2vec_format('vectors.txt', binary=False)\n",
    "        out = w2v.most_similar(positive=positive, negative=negative, topn=topn)\n",
    "        print(positive, negative, out)\n",
    "        \n",
    "        \n",
    "class WordContextMatrixDataset(Sequence):\n",
    "\n",
    "    def __init__(self, lil_mat, positive_samples=10000, negative_samples=5):\n",
    "\n",
    "        self.lil_mat = lil_mat.tolil(copy=True)\n",
    "        # self.lil_mat = np.copy(lil_mat)\n",
    "        self.positive_samples = positive_samples\n",
    "        self.negative_samples = negative_samples\n",
    "        self.batch_size = positive_samples + negative_samples\n",
    "        self.total_samples = self.lil_mat.sum()\n",
    "        self.batches = np.ceil(self.total_samples / self.batch_size).astype(np.int32)\n",
    "        self.last_batch_size = np.mod(self.total_samples, self.batch_size)\n",
    "        self.vocabulary_size = self.lil_mat.shape[0]\n",
    "\n",
    "        # Create a copy of the matrix to restore\n",
    "        self.lil_mat_restore = self.lil_mat.tolil(copy=True)\n",
    "        # self.lil_mat_restore = np.copy(self.lil_mat)\n",
    "        \n",
    "        # Nonzero index matrix and index of nonzero index matrix\n",
    "        self.nonzero_indices = np.array(self.lil_mat.nonzero()).T # Transpose = zip!\n",
    "        self.nonzero_indices_indices = np.array(np.arange(self.nonzero_indices.shape[0]))\n",
    "        \n",
    "    def restore_mat(self):\n",
    "        self.lil_mat = self.lil_mat_restore.tolil(copy=True)\n",
    "        # self.lil_mat = np.copy(self.lil_mat_restore)\n",
    "\n",
    "    def get_batch(self, idx):\n",
    "\n",
    "        # If we already drew all samples, restore everything and restart\n",
    "        if self.nonzero_indices_indices.shape[0] == 0:\n",
    "            self.restore_mat()\n",
    "            self.nonzero_indices_indices = np.array(np.arange(self.nonzero_indices.shape[0]))\n",
    "\n",
    "        # Get batch_size samples, or whatever remains\n",
    "        if self.batch_size > self.nonzero_indices_indices.shape[0]:\n",
    "            sample_size = self.nonzero_indices_indices.shape[0]\n",
    "        else:\n",
    "            sample_size = self.batch_size\n",
    "        negative_sample_size = sample_size * self.negative_samples\n",
    "\n",
    "        # Shuffeling takes place here!\n",
    "        draw = np.random.choice(self.nonzero_indices_indices, size=sample_size, replace=False)\n",
    "        ii = self.nonzero_indices[draw].T[0]\n",
    "        pos_jj = self.nonzero_indices[draw].T[1]\n",
    "        \n",
    "        # Subsampling\n",
    "        if sampling_factor > 0:\n",
    "            rr = np.random.random(size=ii.shape[0])\n",
    "            mm = np.array(sampling_table[ii+1] > rr)\n",
    "            ii = ii[mm]\n",
    "            pos_jj=pos_jj[mm]\n",
    "            sample_size = ii.shape[0]\n",
    "            negative_sample_size = sample_size * self.negative_samples\n",
    "\n",
    "        words = np.zeros(sample_size + negative_sample_size, dtype=np.int32)\n",
    "        words = np.tile(ii, 1 + self.negative_samples)\n",
    "        contexts = np.zeros(sample_size + negative_sample_size, dtype=np.int32)\n",
    "        labels = np.zeros(sample_size + negative_sample_size, dtype=np.int32)\n",
    "\n",
    "        # Positive contexts and labels\n",
    "        ones = np.ones(sample_size, dtype=np.int32)\n",
    "        contexts[0:sample_size] = pos_jj\n",
    "        labels[0:sample_size] = ones\n",
    "\n",
    "        # Negative contexts and labels\n",
    "        neg_jj = np.random.randint(self.vocabulary_size, size=negative_sample_size)\n",
    "        zeros = np.zeros(negative_sample_size)\n",
    "        contexts[sample_size:] = neg_jj\n",
    "        labels[sample_size:] = zeros\n",
    "\n",
    "        # Deduct 1 from every positive sample in the matrix\n",
    "        samples = self.lil_mat[ii, pos_jj].toarray()\n",
    "        # samples = self.lil_mat[ii, pos_jj]\n",
    "        samples-=1\n",
    "\n",
    "        # \"Delete\" sample by deleting (=boolean masking) the index to its index\n",
    "        delete = np.where(samples == 0)[0]\n",
    "        self.nonzero_indices_indices = np.delete(self.nonzero_indices_indices, delete)\n",
    "        \n",
    "        # \"Keep\" sample by just reducing its value in the matrix\n",
    "        keep = np.where(samples > 0)[0]\n",
    "        for k in keep:\n",
    "            self.lil_mat[ii[k], pos_jj[k]]-=1\n",
    "        # self.lil_mat[ii[keep], pos_jj[keep]]-=1 # Not working with LIL\n",
    "\n",
    "        # Prepare data for training\n",
    "        x = [words, contexts]\n",
    "        return x, labels\n",
    "         \n",
    "    def __len__(self):\n",
    "        return self.batches\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.get_batch(idx)\n",
    "        return np.array(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already shuffle within the generator\n",
    "data = WordContextMatrixDataset(lil_mat, positive_samples, negative_samples)\n",
    "c = TestCallback(testword='she')\n",
    "sgns.fit_generator(data, epochs=epochs, shuffle=False, callbacks=[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she'] [] [('her', 0.8064073324203491), ('for', 0.7562803030014038), ('was', 0.7496068477630615), ('it', 0.7416568994522095), ('i', 0.736336350440979), ('he', 0.730705738067627), ('that', 0.7291724681854248), ('but', 0.721697986125946), ('with', 0.7175629138946533), ('had', 0.7128034234046936), ('as', 0.7115832567214966), ('which', 0.7109073400497437), ('and', 0.7054644227027893), ('not', 0.7006860971450806), ('me', 0.6999820470809937), ('his', 0.673018217086792), ('in', 0.6714975237846375), ('by', 0.6634480953216553), ('my', 0.6605647802352905), ('a', 0.6480237245559692)]\n"
     ]
    }
   ],
   "source": [
    "c.most_similar(positive=['she'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['albertine', 'charlus'] ['i'] [('m', 0.4291450083255768), ('de', 0.35334938764572144), ('andree', 0.26091015338897705), ('guermantes', 0.22548697888851166), ('charluss', 0.2221934050321579), ('morel', 0.21699000895023346), ('bore', 0.21379664540290833), ('albertines', 0.21305590867996216), ('villeparisis', 0.20184263586997986), ('pleasure', 0.20126578211784363)]\n"
     ]
    }
   ],
   "source": [
    "c.most_similar(positive=['albertine', 'charlus'], negative=['i'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
